{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wYbhesz5Uh5z",
        "outputId": "99b88f30-681c-4795-b96b-0daee380a16a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at drive; to attempt to forcibly remount, call drive.mount(\"drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/drive/MyDrive && wget https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d1GMZ0xvUoMh",
        "outputId": "52b01868-71e4-42f9-ee45-8ca0d59c8f00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-11 18:07:37--  https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/64878964/b0afbae8-5983-11e6-90f4-e3db656bd548?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220111T180737Z&X-Amz-Expires=300&X-Amz-Signature=521500b5b2f22d2c6bb6558063c4ea9747f3924b7171a534184e0ad5ca770fb6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=64878964&response-content-disposition=attachment%3B%20filename%3Dvgg16_weights_tf_dim_ordering_tf_kernels.h5&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-01-11 18:07:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/64878964/b0afbae8-5983-11e6-90f4-e3db656bd548?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220111%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220111T180737Z&X-Amz-Expires=300&X-Amz-Signature=521500b5b2f22d2c6bb6558063c4ea9747f3924b7171a534184e0ad5ca770fb6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=64878964&response-content-disposition=attachment%3B%20filename%3Dvgg16_weights_tf_dim_ordering_tf_kernels.h5&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 553467096 (528M) [application/octet-stream]\n",
            "Saving to: ‘vgg16_weights_tf_dim_ordering_tf_kernels.h5’\n",
            "\n",
            "vgg16_weights_tf_di 100%[===================>] 527.83M  5.14MB/s    in 67s     \n",
            "\n",
            "2022-01-11 18:08:45 (7.85 MB/s) - ‘vgg16_weights_tf_dim_ordering_tf_kernels.h5’ saved [553467096/553467096]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "## Loading images and labels\n",
        "(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\n",
        "    \"tf_flowers\",\n",
        "    split=[\"train[:70%]\", \"train[:30%]\"], ## Train test split\n",
        "    batch_size=-1,\n",
        "    as_supervised=True,  # Include labels\n",
        ")\n",
        "\n",
        "## Resizing images\n",
        "train_ds = tf.image.resize(train_ds, (150, 150))\n",
        "test_ds = tf.image.resize(test_ds, (150, 150))\n",
        "\n",
        "## Transforming labels to correct format\n",
        "train_labels = to_categorical(train_labels, num_classes=5)\n",
        "test_labels = to_categorical(test_labels, num_classes=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Gup37WGoepfk",
        "outputId": "371462f3-67be-450a-9029-b1c2b4542d6e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.get_single_element()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_datasets/core/dataset_builder.py:598: get_single_element (from tensorflow.python.data.experimental.ops.get_single_element) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.get_single_element()`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "## Loading VGG16 model\n",
        "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
        "base_model.trainable = False ## Not trainable weights\n",
        "\n",
        "## Preprocessing input\n",
        "train_ds = preprocess_input(train_ds) \n",
        "test_ds = preprocess_input(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "UmDId8TTfIK3",
        "outputId": "79d9da9c-2bbd-4a1a-ed64-47598c2ca69f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n",
            "58900480/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rJengRW6fZfQ",
        "outputId": "aef7b3dd-e099-4d52-d601-2a07975f7df1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 0\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "flatten_layer = layers.Flatten()\n",
        "dense_layer_1 = layers.Dense(50, activation='relu')\n",
        "dense_layer_2 = layers.Dense(20, activation='relu')\n",
        "prediction_layer = layers.Dense(5, activation='softmax')\n",
        "\n",
        "\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    flatten_layer,\n",
        "    dense_layer_1,\n",
        "    dense_layer_2,\n",
        "    prediction_layer\n",
        "])"
      ],
      "metadata": {
        "id": "pqPRPgEPfeWz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
        "\n",
        "model.fit(train_ds, train_labels, epochs=3, validation_split=0.2, batch_size=100, callbacks=[es])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A8tkXe1qfkWK",
        "outputId": "64857a4c-8d4f-4996-9b43-8cf16f086a2e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "21/21 [==============================] - 541s 26s/step - loss: 1.2782 - accuracy: 0.5290 - val_loss: 1.0454 - val_accuracy: 0.5798\n",
            "Epoch 2/3\n",
            "21/21 [==============================] - 543s 26s/step - loss: 0.8253 - accuracy: 0.6783 - val_loss: 1.0080 - val_accuracy: 0.6323\n",
            "Epoch 3/3\n",
            "21/21 [==============================] - 545s 26s/step - loss: 0.5621 - accuracy: 0.7844 - val_loss: 0.9922 - val_accuracy: 0.6887\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7efdcdab3f90>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_ds, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9m5MStRkoVQ0",
        "outputId": "14a2a899-a6a3-4c16-a423-cd47f630800e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35/35 [==============================] - 237s 7s/step - loss: 0.4200 - accuracy: 0.8365\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.4200041890144348, 0.8365122675895691]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mGyQPYbfHG-B"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flower categories.\n",
        "categories = ['daisy','dandelion', 'rose', 'sunflower','tulip']"
      ],
      "metadata": {
        "id": "I6jWmERDJmzR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load an image from file\n",
        "img_path = '/content/drive/MyDrive/flowers/daisy/34720703615_bdf1335d8b_n.jpg'\n",
        "image = load_img(img_path, target_size=(150, 150))"
      ],
      "metadata": {
        "id": "kZtw5o3jMKmY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the image pixels into a numpy array and reshape it for the model\n",
        "image = img_to_array(image)\n",
        "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))"
      ],
      "metadata": {
        "id": "10C78H5UpQQv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict the probability \n",
        "predict_prob = model.predict(image)\n",
        "print(categories[np.argmax(predict_prob)])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-RHo61D7Ugvo",
        "outputId": "37caa223-5449-4dd2-8444-e6d3568fe3d1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "daisy\n"
          ]
        }
      ]
    }
  ]
}